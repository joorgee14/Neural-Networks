{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Device Setup\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2. Dataset Class\n",
    "# ---------------------------\n",
    "class ISIC_HDF5_Dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, hdf5_path: str, transform=None, is_labelled: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "        self.hdf5_file = None  # Add this\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.hdf5_file is None:\n",
    "            self.hdf5_file = h5py.File(self.hdf5_path, 'r')  # Open once per worker\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "        encoded_bytes = self.hdf5_file[isic_id][()]\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_rgb)\n",
    "            image = augmented[\"image\"]\n",
    "        else:\n",
    "            image = torch.from_numpy(image_rgb).permute(2, 0, 1).float()\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = torch.tensor(row[\"target\"]).float()\n",
    "            return image, label, isic_id\n",
    "        else:\n",
    "            return image, isic_id\n",
    "\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        with h5py.File(self.hdf5_path, 'r') as hf:\n",
    "            encoded_bytes = hf[isic_id][()]\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb\n",
    "\n",
    "# 3. Load CSVs and Partition Dataset\n",
    "# ---------------------------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = '/content/drive/My Drive/KaggleChallenge'\n",
    "\n",
    "TRAIN_CSV = path+\"/new-train-metadata.csv\"\n",
    "TEST_CSV  = path+\"/students-test-metadata.csv\"\n",
    "TRAIN_HDF5 = path+\"/train-image.hdf5\"\n",
    "TEST_HDF5  = path+\"/test-image.hdf5\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_sub, valid_df_sub = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=42)\n",
    "\n",
    "# 4. Data Augmentation with Albumentations\n",
    "# ---------------------------\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224,224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.8),  # More aggressive rotate/scale\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7053838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset Instantiation\n",
    "# ---------------------------\n",
    "train_dataset = ISIC_HDF5_Dataset(train_df_sub, TRAIN_HDF5, transform=train_transform, is_labelled=True)\n",
    "valid_dataset = ISIC_HDF5_Dataset(valid_df_sub, TRAIN_HDF5, transform=valid_transform, is_labelled=True)\n",
    "test_dataset  = ISIC_HDF5_Dataset(test_df, TEST_HDF5, transform=valid_transform, is_labelled=False)\n",
    "\n",
    "# 6. Weighted Sampler to Balance Classes\n",
    "# ---------------------------\n",
    "class_counts = train_df_sub['target'].value_counts()\n",
    "weights = train_df_sub['target'].apply(lambda x: 1.0 / class_counts[x])\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=6000, replacement=True)\n",
    "\n",
    "# 7. DataLoaders\n",
    "# ---------------------------\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')  # important: no reduction here!\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)  # p_t: prob of true class\n",
    "        loss = (1 - p_t) ** self.gamma * bce_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model: EfficientNet + Dropout\n",
    "# ---------------------------\n",
    "!pip install efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
    "model._fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(model._fc.in_features, 1)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 9. Optimizer, Loss, Scheduler\n",
    "# ---------------------------\n",
    "criterion = FocalLoss(gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96857712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "PATIENCE = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "oof_preds = np.zeros(len(train_df))\n",
    "oof_targets = train_df[\"target\"].values\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df[\"target\"])):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    train_df_sub = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df_sub = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Datasets and Dataloaders\n",
    "    train_dataset = ISIC_HDF5_Dataset(train_df_sub, TRAIN_HDF5, transform=train_transform)\n",
    "    valid_dataset = ISIC_HDF5_Dataset(valid_df_sub, TRAIN_HDF5, transform=valid_transform)\n",
    "\n",
    "    class_counts = train_df_sub['target'].value_counts()\n",
    "    weights = train_df_sub['target'].apply(lambda x: 1.0 / class_counts[x])\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=6000, replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
    "    model._fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(model._fc.in_features, 1))\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = FocalLoss(gamma=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=2, factor=0.5)\n",
    "\n",
    "    best_auc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for images, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images).view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_logits, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in valid_loader:\n",
    "                images = images.to(device)\n",
    "                logits = model(images).view(-1)\n",
    "                val_logits.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_labels.extend(labels.numpy())\n",
    "\n",
    "        val_auc = roc_auc_score(val_labels, val_logits)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss={np.mean(train_losses):.4f}, Val AUC={val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), f\"best_model_fold{fold}.pt\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save OOF predictions\n",
    "    oof_preds[val_idx] = val_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f91295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Inference using all folds\n",
    "# -----------------------------\n",
    "all_fold_preds = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f\"Loading model from fold {fold}...\")\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
    "    model._fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(model._fc.in_features, 1))\n",
    "    model.load_state_dict(torch.load(f\"best_model_fold{fold}.pt\"))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fold_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, isic_ids in tqdm(test_loader, desc=f\"Inference Fold {fold}\"):\n",
    "            images = images.to(device)\n",
    "            probs = torch.sigmoid(model(images).view(-1)).cpu().numpy()\n",
    "            fold_preds.extend(probs)\n",
    "\n",
    "    all_fold_preds.append(fold_preds)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Average predictions across all folds\n",
    "avg_preds = np.mean(all_fold_preds, axis=0)\n",
    "\n",
    "# Prepare submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"isic_id\": [id for _, id in test_dataset],\n",
    "    \"target\": avg_preds\n",
    "})\n",
    "submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "submission_df.to_csv(path + \"/submission_cv_ensemble.csv\", index=False)\n",
    "print(\"Ensemble submission saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
