# Neural-Networks

This coursework focused on:

Developing solutions using Neural Networks with PyTorch

Gaining deep knowledge of deep learning and various network architectures

Solving tasks using MLPs, CNNs, and RNNs

Learning about Transformers, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models

The course included four projects, two partial exams, a Kaggle competition, and a final exam.

Project 1: Autoencoder
Designed and trained Autoencoders on the MNIST and FMNIST datasets, exploring different architectures and regularization techniques, and analyzing their effects.

Project 2: Network Calibration
Based on the paper "On Calibration of Modern Neural Networks", trained a LeNet-5 model on a modified CIFAR-10 dataset, studied calibration curves, implemented Temperature Scaling, and optionally tested with deeper networks.

Project 3: Attention Mechanism
Implemented an attention mechanism from scratch for an LSTM-based RNN, following specific design instructions, and compared its performance against a standard LSTM without attention.

Project 4: Variational Autoencoder (VAE)
Completed the implementation of a VAE for the CelebA dataset using convolutional networks. Additionally, built a VAE for synthetic data from a 3D Gaussian Mixture Model using dense layers. Compared generated samples to the ground truth, evaluated missing modes, and used T-SNE to visualize and analyze clustering behavior in the latent space.
